{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf13044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os                      # For environment variables\n",
    "from dotenv import load_dotenv # For loading .env file (API keys)\n",
    "from langchain_community.vectorstores import Chroma         # Chroma vector DB\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI   # For LLM/embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate       # Prompt templating\n",
    "\n",
    "# Load secrets (should include OPENAI_API_KEY, etc.)\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d4538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "CHROMA_PATH = \"chroma\"    # Path to your local Chroma DB\n",
    "\n",
    "# 1. Create an embedding model (assumes OPENAI_API_KEY in .env)\n",
    "embedding_function = OpenAIEmbeddings(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# 2. Load the persistent Chroma vector DB (for semantic search)\n",
    "db = Chroma(\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    embedding_function=embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8131803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def rag_query(question: str, similarity_threshold: float = 0.5) -> Dict:\n",
    "    \"\"\"\n",
    "    Core RAG pipeline.\n",
    "    - Retrieves relevant docs from vector DB using embeddings.\n",
    "    - Builds context-aware prompt.\n",
    "    - Calls chat LLM to generate answer.\n",
    "    - Returns answer and document sources.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Semantic search in your vector DB\n",
    "    results = db.similarity_search_with_relevance_scores(question, k=3)\n",
    "    if not results or results[0][1] < similarity_threshold:\n",
    "        return {\"response\": None, \"sources\": [], \"found\": False}\n",
    "\n",
    "    # 2. Build the context for the prompt\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "\n",
    "    # 3. Format the full prompt\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "\n",
    "    # 4. Instantiate the LLM and ask the question\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "        model=\"gpt-4o-mini\",  # Or any model you have access to\n",
    "    )\n",
    "    response_text = llm.predict(prompt)\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _ in results]\n",
    "\n",
    "    return {\n",
    "        \"response\": response_text,\n",
    "        \"sources\": sources,\n",
    "        \"context\": context_text,\n",
    "        \"found\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f77e47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\Evan\\AppData\\Local\\Temp\\ipykernel_2108\\1780745097.py:39: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response_text = llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " No, Alice does not fall through the earth. She wonders how far she has fallen and speculates about being near the center of the earth, but the context does not indicate that she actually falls through it.\n",
      "\n",
      "Sources:\n",
      " ['data\\\\books\\\\alice_in_wonderland.md', 'data\\\\books\\\\alice_in_wonderland.md', 'data\\\\books\\\\alice_in_wonderland.md']\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Does Alice falls through the earth\"\n",
    "\n",
    "result = rag_query(user_question)\n",
    "if result[\"found\"]:\n",
    "    print(\"Answer:\\n\", result[\"response\"])\n",
    "    print(\"\\nSources:\\n\", result[\"sources\"])\n",
    "else:\n",
    "    print(\"No relevant context found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
